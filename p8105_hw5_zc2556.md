P8105-hw5-zc2556
================
Zhe Chen
2020/11/17

## Problem 1

### Libraries and Basics

### Import and Clean the Data Set

``` r
#read in the dataset
homoicide = 
  read_csv("./homicide-data.csv") %>%
  #redefine the arrest status
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>%
  select(
    city_state,resolved
  ) %>%
  filter(city_state != "Tulsa_AL")
```

    ## Parsed with column specification:
    ## cols(
    ##   uid = col_character(),
    ##   reported_date = col_double(),
    ##   victim_last = col_character(),
    ##   victim_first = col_character(),
    ##   victim_race = col_character(),
    ##   victim_age = col_character(),
    ##   victim_sex = col_character(),
    ##   city = col_character(),
    ##   state = col_character(),
    ##   lat = col_double(),
    ##   lon = col_double(),
    ##   disposition = col_character()
    ## )

Check the data set

``` r
#give a tidy data frame including the solved/unsolved cases 
aggregate_homo = 
  homoicide %>%
  group_by(city_state) %>%
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
aggregate_homo
```

    ## # A tibble: 50 x 3
    ##    city_state     hom_total hom_unsolved
    ##    <chr>              <int>        <int>
    ##  1 Albuquerque_NM       378          146
    ##  2 Atlanta_GA           973          373
    ##  3 Baltimore_MD        2827         1825
    ##  4 Baton Rouge_LA       424          196
    ##  5 Birmingham_AL        800          347
    ##  6 Boston_MA            614          310
    ##  7 Buffalo_NY           521          319
    ##  8 Charlotte_NC         687          206
    ##  9 Chicago_IL          5535         4073
    ## 10 Cincinnati_OH        694          309
    ## # ... with 40 more rows

Prop test for a single city

``` r
prop.test(
  aggregate_homo %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
  aggregate_homo %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)
  ) %>%
  broom::tidy() #make it exhibit in one row
```

    ## # A tibble: 1 x 8
    ##   estimate statistic  p.value parameter conf.low conf.high method    alternative
    ##      <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>     <chr>      
    ## 1    0.646      239. 6.46e-54         1    0.628     0.663 1-sample~ two.sided

Iterate

``` r
#using map to give prop tests of each city state
results =
  aggregate_homo %>%
  mutate(
    prop_test = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_test = map(.x = prop_test, ~broom::tidy(.x))
  ) %>%
  select(
    -prop_test
  ) %>%
  unnest(tidy_test) %>%
  select(
    city_state, estimate, conf.low, conf.high
  )
results
```

    ## # A tibble: 50 x 4
    ##    city_state     estimate conf.low conf.high
    ##    <chr>             <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque_NM    0.386    0.337     0.438
    ##  2 Atlanta_GA        0.383    0.353     0.415
    ##  3 Baltimore_MD      0.646    0.628     0.663
    ##  4 Baton Rouge_LA    0.462    0.414     0.511
    ##  5 Birmingham_AL     0.434    0.399     0.469
    ##  6 Boston_MA         0.505    0.465     0.545
    ##  7 Buffalo_NY        0.612    0.569     0.654
    ##  8 Charlotte_NC      0.300    0.266     0.336
    ##  9 Chicago_IL        0.736    0.724     0.747
    ## 10 Cincinnati_OH     0.445    0.408     0.483
    ## # ... with 40 more rows

Plot

``` r
results %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point()+
  geom_errorbar(ymin = results$conf.low, ymax = results$conf.high)+
  theme(axis.text = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw5_zc2556_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />

## Problem 2

### Import Data Sets with Iteration

``` r
#store file paths of the control data sets
path_con = 
  str_c("./data/", list.files("data", pattern = "con"))

#store file paths of the experiment data sets 
path_exp = 
  str_c("./data/", list.files("data", pattern = "exp"))

#import both controls and experiments into a single data set
long_data = 
  tibble(
    control = map(path_con, read_csv),
    experiment = map(path_exp, read_csv)
  )

path_con = 
  str_c("./data/", list.files("data", pattern = "con"))

path_exp = 
  str_c("./data/", list.files("data", pattern = "exp"))

long_data = 
  tibble(
    control = map(path_con, read_csv),
    experiment = map(path_exp, read_csv)
  )
```

### Make the Data Tidy

``` r
#make it tidy
long_data_tidy = 
  long_data %>%
  mutate(
    subject_id = 1:10
  ) %>%
  pivot_longer(
    control:experiment,
    names_to = "arm"
  ) %>%
  unnest() %>%
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observations"
  )
```

    ## Warning: `cols` is now required when using unnest().
    ## Please use `cols = c(value)`

``` r
long_data_tidy
```

    ## # A tibble: 160 x 4
    ##    subject_id arm        week  observations
    ##         <int> <chr>      <chr>        <dbl>
    ##  1          1 control    1             0.2 
    ##  2          1 control    2            -1.31
    ##  3          1 control    3             0.66
    ##  4          1 control    4             1.96
    ##  5          1 control    5             0.23
    ##  6          1 control    6             1.09
    ##  7          1 control    7             0.05
    ##  8          1 control    8             1.94
    ##  9          1 experiment 1             3.05
    ## 10          1 experiment 2             3.67
    ## # ... with 150 more rows

### Plot

``` r
long_data_tidy %>%
  mutate(subject_id = as.factor(subject_id),
         week = as.factor(week)
         ) %>%
  ggplot(aes(x = week, y = observations, group = subject_id, color = subject_id)) +
  geom_path()+
  facet_grid(~arm) +
  labs(
    title = "Observations over 8 Weeks by Treatment Groups",
    x = "Week",
    y = "Observations"
  ) + 
  theme(legend.position = "bottom")
```

<img src="p8105_hw5_zc2556_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

We made a spaghetti plot to show the observations on each subject over
time to compare the effect of the treatment. In control group, we donâ€™t
observe a strong trend and the change of observations for 8 weeks seems
to be random. However, for the experiment group, we can observe a
clearly increasing tread, showing the treatment should have some effect
compared to the control.

## Problem 3

``` r
#set up 
set.seed(621)
n_ent = 30
sigm_ent = 5

#function to find the estimated mean and p-value
sim_mean_p = function(n, mu, sigm) {
  x = rnorm(n, mean = mu, sd = sigm)
  t = broom::tidy(t.test(x))
  fun_res = tibble(
    mu_hat = t$estimate,
    p_value = t$p.value
    )
  return(fun_res)
}
```

5000 for mu = 0

``` r
#function to stimulate for 5000 times 
stimulate = function(mu) {
  output_mu= vector("list", 5000)
  for(i in 1:5000) {
    output_mu[[i]] = sim_mean_p(n_ent, mu, sigm_ent)
  }
  sim_result = bind_rows(output_mu)
  return(sim_result)
}

#when mu = 0
mu0 = stimulate(0)
mu0
```

    ## # A tibble: 5,000 x 2
    ##    mu_hat p_value
    ##     <dbl>   <dbl>
    ##  1  0.432   0.632
    ##  2 -1.32    0.230
    ##  3  0.254   0.804
    ##  4  0.245   0.808
    ##  5  1.03    0.309
    ##  6 -0.488   0.511
    ##  7 -0.593   0.545
    ##  8  0.351   0.696
    ##  9  0.393   0.632
    ## 10  0.630   0.489
    ## # ... with 4,990 more rows

Proportion of Rejecting the Null based on 0.05 alpha when mu = 0.

``` r
#function to find the proportion of rejecting the null
test = function(stimulate_data, true_mu) {
  sim_reject = 
    stimulate_data %>%
      filter(p_value < 0.05) %>%
      summarise(
        prop_reject = n()/5000
      )
  mu =
    tibble(
      true_mu = true_mu
    )
  sim_reject = bind_cols(sim_reject, mu)
  return(sim_reject)
}

#for mu = 0 
test(mu0,0)
```

    ## # A tibble: 1 x 2
    ##   prop_reject true_mu
    ##         <dbl>   <dbl>
    ## 1      0.0518       0

### Plot the Proportion of Reject VS True Mean

Apply functions

``` r
#make a data frame including mu = 1-6
for (i in 1:6) {
  assign(paste0("mu", i), stimulate(i))
}
```

Make a table for plotting

``` r
#make a data frames including the proportion and true means
mu_all = list(mu0,mu1,mu2,mu3,mu4,mu5,mu6)
for (i in 1:7) {
  assign(paste0("prop_table", i), test(as.data.frame(mu_all[i]),i))
}
prop_table = bind_rows(prop_table1, prop_table2, prop_table3, prop_table4, prop_table5, prop_table6, prop_table7) %>%
  mutate(
    true_mu = true_mu - 1
  )
prop_table
```

    ##   prop_reject true_mu
    ## 1      0.0518       0
    ## 2      0.1834       1
    ## 3      0.5628       2
    ## 4      0.8818       3
    ## 5      0.9874       4
    ## 6      0.9998       5
    ## 7      1.0000       6

Plot

``` r
prop_table %>%
  ggplot(aes(x = as.factor(true_mu), y = prop_reject, group  = 1)) +
  geom_point() +
  geom_line() +
  ylim (0,1) +
  geom_text(aes(label = prop_reject, hjust = 0, vjust = 0.5, angle = -25))+
  labs(
    title = "5000 Times of Stimulation for 6 Means",
    x = "True Mean",
    y = "Proportion of Rejection Times"
  ) + 
  
  theme(legend.position = "bottom")
```

<img src="p8105_hw5_zc2556_files/figure-gfm/unnamed-chunk-15-1.png" width="90%" />

From the plot, we can observe a increasing trend while the true mean
increases from 0 to 6 and the proportion of rejection times approaches
to 1. From the plot, we can conclude that the much more different of the
alternative hypothesis (larger mean in this case), the more accurate of
the test (greater proportion of rejection times). While the mean of the
tested mean is the same as the null, the proportion of rejection times
is pretty close to the level of significance (0.05 in our case).

### Plot the Average Estimate of mu VS True Mean

``` r
#find the average estimate of mu
find_av = function(stimulate_data, true_mu) {
  av_est = 
    stimulate_data %>%
      summarise(
        mu_bar = round(mean(mu_hat),digits = 4)
      )
  mu =
    tibble(
      true_mu = true_mu
    )
  av_est = bind_cols(av_est, mu)
  return(av_est)
}

#create table of the average of estimates
for (i in 1:7) {
  assign(paste0("av_est", i), find_av(as.data.frame(mu_all[i]),i))
}
av_est = bind_rows(av_est1, av_est2, av_est3, av_est4, av_est5, av_est6, av_est7) %>%
  mutate(
    true_mu = true_mu - 1
  )
av_est  
```

    ##   mu_bar true_mu
    ## 1 0.0453       0
    ## 2 1.0005       1
    ## 3 1.9978       2
    ## 4 2.9904       3
    ## 5 3.9825       4
    ## 6 4.9969       5
    ## 7 6.0095       6

Plot

``` r
av_est %>%
  ggplot(aes(x = as.factor(true_mu), y = mu_bar, group  = 1)) +
  geom_point() +
  geom_line() +
  geom_text(aes(label = mu_bar, hjust = 0, vjust = 1.1))+
  labs(
    title = "5000 Times of Stimulation for 6 Means",
    x = "True Mean",
    y = "Average of Estimated Mean"
  ) + 
  
  theme(legend.position = "bottom")
```

<img src="p8105_hw5_zc2556_files/figure-gfm/unnamed-chunk-17-1.png" width="90%" />

From the plot, we can clearly observe a nearly perfect linear relation
between the average of estimated means and the true mean, showing that
the average of estimated means is equal to the true mean. Thus, we are
confident to estimate the true mean with the sample mean when the number
of simulation times is large enough.

### Plot the Average Estimate (rejected) of mu VS True Mean

``` r
#find the average estimate of mu (rejected)
find_av_rej = function(stimulate_data, true_mu) {
  av_est_rej = 
    stimulate_data %>%
      filter(p_value < 0.05) %>%
      summarise(
        mu_bar_rej = round(mean(mu_hat),digits = 4)
      )
  mu =
    tibble(
      true_mu = true_mu
    )
  av_est_rej = bind_cols(av_est_rej, mu)
  return(av_est_rej)
}

#create table of the average of estimates (rejected)
for (i in 1:7) {
  assign(paste0("av_est_rej", i), find_av_rej(as.data.frame(mu_all[i]),i))
}
av_est_rej = bind_rows(av_est_rej1, av_est_rej2, av_est_rej3, av_est_rej4, av_est_rej5, av_est_rej6, av_est_rej7) %>%
  mutate(
    true_mu = true_mu - 1
  )
av_est_rej  
```

    ##   mu_bar_rej true_mu
    ## 1     0.1181       0
    ## 2     2.2457       1
    ## 3     2.6108       2
    ## 4     3.1898       3
    ## 5     4.0117       4
    ## 6     4.9976       5
    ## 7     6.0095       6

Plot

``` r
av_est_rej %>%
  ggplot(aes(x = as.factor(true_mu), y = mu_bar_rej, group  = 1)) +
  geom_point() +
  geom_line() +
  geom_text(aes(label = mu_bar_rej, hjust = 0, vjust = 1.1))+
  labs(
    title = "5000 Times of Stimulation for 6 Means (Rejected)",
    x = "True Mean",
    y = "Average of Estimated Mean (Rejected)"
  ) + 
  theme(legend.position = "bottom")
```

<img src="p8105_hw5_zc2556_files/figure-gfm/unnamed-chunk-19-1.png" width="90%" />

From the plot, we donâ€™t observe a nearly perfect linear relation between
the rejected average of estimated means and the true mean when
